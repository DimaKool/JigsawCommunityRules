{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "308808fd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-24T20:02:40.003359Z",
     "iopub.status.busy": "2025-09-24T20:02:40.002660Z",
     "iopub.status.idle": "2025-09-24T20:02:41.277637Z",
     "shell.execute_reply": "2025-09-24T20:02:41.276891Z"
    },
    "papermill": {
     "duration": 1.280066,
     "end_time": "2025-09-24T20:02:41.279385",
     "exception": false,
     "start_time": "2025-09-24T20:02:39.999319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/compressed_tensors-0.10.2-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/__script__.py\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/gekko-1.3.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/auto_gptq-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/shellingham-1.5.4-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/outlines_core-0.2.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/typing_extensions-4.15.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/msgpack-1.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/httpx-0.28.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/requests-2.32.5-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pybase64-1.4.2-cp311-cp311-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/fastapi_cli-0.0.13-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pandas-2.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/optimum-1.27.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/fastrlock-0.8.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pyzmq-27.1.0-cp311-cp311-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/typer-0.19.2-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/py_cpuinfo-9.0.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/h11-0.16.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/lark-1.2.2-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pycountry-24.6.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/rich_toolkit-0.15.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/httpcore-1.0.9-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/jsonschema_specifications-2025.9.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/aiohttp-3.12.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/interegular-0.3.3-py37-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/jsonschema-4.25.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/depyf-0.19.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/regex-2025.9.18-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pydantic-2.11.9-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/wcwidth-0.2.14-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/logits_processor_zoo-0.2.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/partial_json_parser-0.2.1.1.post6-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/lm_format_enforcer-0.10.12-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/python_dotenv-1.1.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/rouge-1.0.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/tqdm-4.67.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/emoji-1.7.0.tar.gz\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/__results__.html\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/networkx-3.5-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/transformers-4.56.2-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/openai-1.90.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/dill-0.4.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/mistral_common-1.8.5-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/multiprocess-0.70.16-py311-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/python_multipart-0.0.20-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/annotated_types-0.7.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pytz-2025.2-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/filelock-3.19.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/dnspython-2.8.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/sentry_sdk-2.38.0-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/idna-3.10-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/prometheus_client-0.23.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/urllib3-2.5.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/mpmath-1.3.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/cffi-2.0.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/aiohappyeyeballs-2.6.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/sniffio-1.3.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/jinja2-3.1.6-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/prometheus_fastapi_instrumentator-7.1.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pydantic_extra_types-2.10.5-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/charset_normalizer-3.4.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/jiter-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/click-8.3.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/input_requirements.txt\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/vllm-0.10.0-cp38-abi3-manylinux1_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/websockets-15.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/aiosignal-1.4.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/six-1.17.0-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/typing_inspection-0.4.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/blake3-1.0.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/referencing-0.36.2-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/ray-2.49.2-cp311-cp311-manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/trl-0.21.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/anyio-4.11.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/yarl-1.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/python_json_logger-3.3.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/clean_text-0.6.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/email_validator-2.3.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/gguf-0.17.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/deepspeed-0.17.4.tar.gz\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/python_dateutil-2.9.0.post0-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/fastapi_cloud_cli-0.2.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/fsspec-2025.9.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pygments-2.19.2-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/attrs-25.3.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/datasets-4.1.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/cloudpickle-3.1.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/distro-1.9.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/rich-14.1.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/uvicorn-0.37.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/peft-0.17.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/__script__.ipynb\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/markdown_it_py-4.0.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/ftfy-6.3.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/fastapi-0.117.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/diskcache-5.6.3-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/llguidance-0.7.30-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/accelerate-1.10.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/packaging-25.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/rpds_py-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/__output__.json\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/setuptools-80.9.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/xgrammar-0.1.21-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/multidict-6.6.4-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/sentencepiece-0.2.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/astor-0.8.1-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/rignore-0.6.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/cachetools-6.2.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/sympy-1.14.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/mdurl-0.1.2-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pycparser-2.23-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/soxr-1.0.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/protobuf-6.32.1-cp39-abi3-manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/cupy_cuda12x-13.6.0-cp311-cp311-manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/cbor2-5.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/hjson-3.1.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/install_requirements.sh\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/certifi-2025.8.3-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/tzdata-2025.2-py2.py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/huggingface_hub-0.35.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/propcache-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/scipy-1.16.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/starlette-0.48.0-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/frozenlist-1.7.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/einops-0.8.1-py3-none-any.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "/kaggle/input/pm-93025403-at-09-24-2025-19-54-00/custom.css\n",
      "/kaggle/input/jigsaw-agile-community-rules/sample_submission.csv\n",
      "/kaggle/input/jigsaw-agile-community-rules/train.csv\n",
      "/kaggle/input/jigsaw-agile-community-rules/test.csv\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/model.safetensors.index.json\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/config.json\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/merges.txt\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/model-00001-of-00002.safetensors\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/model-00002-of-00002.safetensors\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/README.md\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/tokenizer.json\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/vocab.json\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/tokenizer_config.json\n",
      "/kaggle/input/qwen-3/transformers/1.7b/1/generation_config.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0c3386b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:02:41.286379Z",
     "iopub.status.busy": "2025-09-24T20:02:41.286052Z",
     "iopub.status.idle": "2025-09-24T20:02:41.291101Z",
     "shell.execute_reply": "2025-09-24T20:02:41.290474Z"
    },
    "papermill": {
     "duration": 0.008802,
     "end_time": "2025-09-24T20:02:41.292124",
     "exception": false,
     "start_time": "2025-09-24T20:02:41.283322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constants.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants.py\n",
    "BASE_MODEL_PATH = \"/kaggle/input/qwen-3/transformers/1.7b/1\"\n",
    "LORA_PATH = \"output/\"\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\"\n",
    "COMPLETE_PHRASE = \"Answer:\"\n",
    "BASE_PROMPT = '''You are given a comment from reddit and a rule. Your task is to classify whether the comment violates the rule. Only respond Yes/No.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f944cbe7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:02:41.297423Z",
     "iopub.status.busy": "2025-09-24T20:02:41.297211Z",
     "iopub.status.idle": "2025-09-24T20:02:41.302806Z",
     "shell.execute_reply": "2025-09-24T20:02:41.302019Z"
    },
    "papermill": {
     "duration": 0.00967,
     "end_time": "2025-09-24T20:02:41.303952",
     "exception": false,
     "start_time": "2025-09-24T20:02:41.294282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from constants import POSITIVE_ANSWER, NEGATIVE_ANSWER, COMPLETE_PHRASE, BASE_PROMPT\n",
    "import random, numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# COMPLETE_PHRASE = \"Answer:\n",
    "def build_prompt(row):\n",
    "    return f\"\"\"\n",
    "{BASE_PROMPT}\n",
    "\n",
    "Subreddit: r/{row[\"subreddit\"]}\n",
    "Rule: {row[\"rule\"]}\n",
    "Examples:\n",
    "1) {row[\"positive_example\"]}\n",
    "{COMPLETE_PHRASE} Yes\n",
    "\n",
    "2) {row[\"negative_example\"]}\n",
    "{COMPLETE_PHRASE} No\n",
    "\n",
    "---\n",
    "Comment: {row[\"body\"]}\n",
    "{COMPLETE_PHRASE}\"\"\"\n",
    "\n",
    "\n",
    "def get_dataframe_to_train(data_path):\n",
    "    # Only take 50% of the samples\n",
    "    test_dataset = pd.read_csv(f\"{data_path}/test.csv\").sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    flatten = []\n",
    "\n",
    "    # Test set treatment. We create more training examples\n",
    "    for violation_type in [\"positive\", \"negative\"]:\n",
    "        for i in range(1, 3):\n",
    "            sub_dataset = test_dataset[[\"rule\",\"subreddit\",\n",
    "                                        \"positive_example_1\",\"positive_example_2\",\n",
    "                                        \"negative_example_1\",\"negative_example_2\"]].copy()\n",
    "\n",
    "            if violation_type == \"positive\":\n",
    "                # Positive column becomes a body for the train set\n",
    "                body_col = f\"positive_example_{i}\"\n",
    "                other_positive_col = f\"positive_example_{3-i}\"  # other positive example \n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"positive_example\"] = sub_dataset[other_positive_col]\n",
    "                # we take a random negative example between 2 of them\n",
    "                sub_dataset[\"negative_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"negative_example_1\"],\n",
    "                    sub_dataset[\"negative_example_2\"]\n",
    "                )\n",
    "                # Assign the lable positive\n",
    "                sub_dataset[\"rule_violation\"] = 1\n",
    "\n",
    "            else:  # violation_type == \"negative\"\n",
    "                body_col = f\"negative_example_{i}\"\n",
    "                other_negative_col = f\"negative_example_{3-i}\"\n",
    "                sub_dataset[\"body\"] = sub_dataset[body_col]\n",
    "                sub_dataset[\"negative_example\"] = sub_dataset[other_negative_col]\n",
    "                sub_dataset[\"positive_example\"] = np.where(\n",
    "                    np.random.rand(len(sub_dataset)) < 0.5,\n",
    "                    sub_dataset[\"positive_example_1\"],\n",
    "                    sub_dataset[\"positive_example_2\"]\n",
    "                )\n",
    "                sub_dataset[\"rule_violation\"] = 0\n",
    "\n",
    "            # Drop columns and append more samples to our flatten dataframe\n",
    "            sub_dataset.drop(columns=[\"positive_example_1\",\"positive_example_2\",\n",
    "                                      \"negative_example_1\",\"negative_example_2\"], inplace=True)\n",
    "\n",
    "            flatten.append(sub_dataset)\n",
    "\n",
    "    # 合并所有 DataFrame\n",
    "    dataframe = pd.concat(flatten, axis=0)\n",
    "    dataframe = dataframe.drop_duplicates(ignore_index=True)\n",
    "\n",
    "    return dataframe\n",
    "# TODO\n",
    "# Try to have only test samples that have non duplicate examples\n",
    "# Try to change different proportion of test set\n",
    "\n",
    "\n",
    "def build_dataset(dataframe):\n",
    "    # New column for the prompt which is processed by all lines\n",
    "    dataframe[\"prompt\"] = dataframe.apply(build_prompt, axis=1)\n",
    "\n",
    "    columns = [\"prompt\"]\n",
    "    if \"rule_violation\" in dataframe:\n",
    "        dataframe[\"completion\"] = dataframe[\"rule_violation\"].map(\n",
    "            {\n",
    "                1: POSITIVE_ANSWER,\n",
    "                0: NEGATIVE_ANSWER,\n",
    "            }\n",
    "        )\n",
    "        columns.append(\"completion\")\n",
    "\n",
    "    # Keep only prompt and completion\n",
    "    dataframe = dataframe[columns]\n",
    "    dataset = Dataset.from_pandas(dataframe)\n",
    "    dataset.to_pandas().to_csv(\"/kaggle/working/dataset.csv\", index=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1218d119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:02:41.309817Z",
     "iopub.status.busy": "2025-09-24T20:02:41.309483Z",
     "iopub.status.idle": "2025-09-24T20:02:41.314136Z",
     "shell.execute_reply": "2025-09-24T20:02:41.313579Z"
    },
    "papermill": {
     "duration": 0.008823,
     "end_time": "2025-09-24T20:02:41.315169",
     "exception": false,
     "start_time": "2025-09-24T20:02:41.306346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import pandas as pd\n",
    "\n",
    "# high-level trainer optimized for supervised fine-tuning\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# a fine-tuning technique designed to modify the behavior of pre-trained models with minimal computational overhead\n",
    "from peft import LoraConfig\n",
    "from tqdm.auto import tqdm\n",
    "from transformers.utils import is_torch_bf16_gpu_available\n",
    "from utils import build_dataset, get_dataframe_to_train\n",
    "from constants import DATA_PATH, BASE_MODEL_PATH, LORA_PATH\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Data preparation\n",
    "    dataframe = get_dataframe_to_train(DATA_PATH)\n",
    "    train_dataset = build_dataset(dataframe)\n",
    "    # Why LoRA? You keep the base (quantized) model frozen, and train only tiny adapter weights \n",
    "    # → huge memory & speed savings.\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "    training_args = SFTConfig(\n",
    "        num_train_epochs=1,\n",
    "        \n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        learning_rate=1e-4, #keep high, lora usually likes high. \n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=1.0,\n",
    "        \n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        \n",
    "        bf16=is_torch_bf16_gpu_available(),\n",
    "        fp16=not is_torch_bf16_gpu_available(),\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "        gradient_checkpointing=True,\n",
    "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    \n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "    \n",
    "        completion_only_loss=True,\n",
    "        packing=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "    # Initialize training\n",
    "    trainer = SFTTrainer(\n",
    "        BASE_MODEL_PATH,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    trainer.save_model(LORA_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68a6ac6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:02:41.320914Z",
     "iopub.status.busy": "2025-09-24T20:02:41.320696Z",
     "iopub.status.idle": "2025-09-24T20:02:41.326284Z",
     "shell.execute_reply": "2025-09-24T20:02:41.325685Z"
    },
    "papermill": {
     "duration": 0.009724,
     "end_time": "2025-09-24T20:02:41.327332",
     "exception": false,
     "start_time": "2025-09-24T20:02:41.317608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference.py\n",
    "import os\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import vllm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from vllm.lora.request import LoRARequest\n",
    "from utils import build_dataset\n",
    "from constants import BASE_MODEL_PATH, LORA_PATH, DATA_PATH, POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def run_inference_on_device(df_slice):\n",
    "    \"\"\"在当前进程可见的 GPU 上跑 vLLM 推理\"\"\"\n",
    "    llm = vllm.LLM(\n",
    "        BASE_MODEL_PATH,\n",
    "        quantization=\"gptq\",\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.98,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        max_model_len=2836,\n",
    "        disable_log_stats=True,\n",
    "        enable_prefix_caching=True,\n",
    "        enable_lora=True,\n",
    "        max_lora_rank=64,\n",
    "    )\n",
    "\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    # Limit LLM to only use two options to stabilize outputs\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n",
    "\n",
    "    # Build a test dataset\n",
    "    test_dataset = build_dataset(df_slice)\n",
    "    texts = test_dataset[\"prompt\"]\n",
    "\n",
    "    # Only 1 token\n",
    "    # mclp so limited to 2 options\n",
    "    # Return log probailities for both options\n",
    "    outputs = llm.generate(\n",
    "        texts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "        lora_request=LoRARequest(\"default\", 1, LORA_PATH)\n",
    "    )\n",
    "\n",
    "    log_probs = [\n",
    "        {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()}\n",
    "        for out in outputs\n",
    "    ]\n",
    "    # Make a dataframe with id column and log prob for each option to have a submission form\n",
    "    predictions = pd.DataFrame(log_probs)[[POSITIVE_ANSWER, NEGATIVE_ANSWER]]\n",
    "    predictions[\"row_id\"] = df_slice[\"row_id\"].values\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def worker(device_id, df_slice, return_dict):\n",
    "    # We process each slice on 1 GPU\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(device_id)\n",
    "    print(f\"[Worker {device_id}] Running on GPU {device_id}, data size={len(df_slice)}\")\n",
    "\n",
    "    preds = run_inference_on_device(df_slice)\n",
    "    return_dict[device_id] = preds\n",
    "\n",
    "\n",
    "def main():\n",
    "    test_dataframe = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "\n",
    "    # 随机选择例子\n",
    "    test_dataframe[\"positive_example\"] = test_dataframe.apply(\n",
    "        lambda row: random.choice([row[\"positive_example_1\"], row[\"positive_example_2\"]]),\n",
    "        axis=1\n",
    "    )\n",
    "    test_dataframe[\"negative_example\"] = test_dataframe.apply(\n",
    "        lambda row: random.choice([row[\"negative_example_1\"], row[\"negative_example_2\"]]),\n",
    "        axis=1\n",
    "    )\n",
    "    test_dataframe = test_dataframe.drop(\n",
    "        columns=[\"positive_example_1\", \"positive_example_2\", \"negative_example_1\", \"negative_example_2\"],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    # Divide dataset\n",
    "    mid = len(test_dataframe) // 2\n",
    "    df0 = test_dataframe.iloc[:mid].reset_index(drop=True)\n",
    "    df1 = test_dataframe.iloc[mid:].reset_index(drop=True)\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "\n",
    "    # 两个进程并行\n",
    "    p0 = mp.Process(target=worker, args=(0, df0, return_dict))\n",
    "    p1 = mp.Process(target=worker, args=(1, df1, return_dict))\n",
    "    p0.start()\n",
    "    p1.start()\n",
    "    p0.join()\n",
    "    p1.join()\n",
    "\n",
    "    # 合并结果\n",
    "    predictions = pd.concat([return_dict[0], return_dict[1]], ignore_index=True)\n",
    "\n",
    "    # 构建 submission\n",
    "    submission = predictions[[\"row_id\", POSITIVE_ANSWER]].rename(columns={POSITIVE_ANSWER: \"rule_violation\"})\n",
    "    rq = submission['rule_violation'].rank(method='average') / (len(submission) + 1)\n",
    "    submission['rule_violation'] = rq\n",
    "\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"✅ Saved submission_qwen.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9302a9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:02:41.333163Z",
     "iopub.status.busy": "2025-09-24T20:02:41.332671Z",
     "iopub.status.idle": "2025-09-24T20:02:41.337319Z",
     "shell.execute_reply": "2025-09-24T20:02:41.336619Z"
    },
    "papermill": {
     "duration": 0.008795,
     "end_time": "2025-09-24T20:02:41.338493",
     "exception": false,
     "start_time": "2025-09-24T20:02:41.329698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing accelerate_config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile accelerate_config.yaml\n",
    "compute_environment: LOCAL_MACHINE\n",
    "debug: false\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: 4\n",
    "  gradient_clipping: 1.0\n",
    "  train_batch_size: 64\n",
    "  train_micro_batch_size_per_gpu: 4\n",
    "  \n",
    "  zero_stage: 2\n",
    "  offload_optimizer_device: none\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  \n",
    "  stage3_gather_16bit_weights_on_model_save: false\n",
    "  stage3_max_live_parameters: 1e8\n",
    "  stage3_max_reuse_distance: 1e8\n",
    "  stage3_prefetch_bucket_size: 5e7\n",
    "  stage3_param_persistence_threshold: 1e5\n",
    "  \n",
    "  zero_allow_untested_optimizer: true\n",
    "  zero_force_ds_cpu_optimizer: false\n",
    "  \n",
    "  fp16:\n",
    "    enabled: true\n",
    "    loss_scale: 0\n",
    "    initial_scale_power: 16\n",
    "    loss_scale_window: 1000\n",
    "    hysteresis: 2\n",
    "    min_loss_scale: 1\n",
    "  \n",
    "distributed_type: DEEPSPEED\n",
    "downcast_bf16: 'no'\n",
    "dynamo_config:\n",
    "  dynamo_backend: INDUCTOR\n",
    "  dynamo_use_fullgraph: false\n",
    "  dynamo_use_dynamic: false\n",
    "enable_cpu_affinity: false\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "mixed_precision: fp16\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e22b6d99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:02:41.344647Z",
     "iopub.status.busy": "2025-09-24T20:02:41.344440Z",
     "iopub.status.idle": "2025-09-24T20:05:51.598028Z",
     "shell.execute_reply": "2025-09-24T20:05:51.597217Z"
    },
    "papermill": {
     "duration": 190.25832,
     "end_time": "2025-09-24T20:05:51.599697",
     "exception": false,
     "start_time": "2025-09-24T20:02:41.341377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-24 20:02:51,422] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-24 20:02:53,306] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "2025-09-24 20:02:54.680648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758744174.887047     192 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758744174.945724     192 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "W0924 20:03:06.514000 192 torch/distributed/run.py:766] \r\n",
      "W0924 20:03:06.514000 192 torch/distributed/run.py:766] *****************************************\r\n",
      "W0924 20:03:06.514000 192 torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0924 20:03:06.514000 192 torch/distributed/run.py:766] *****************************************\r\n",
      "[W924 20:03:14.826022895 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W924 20:03:22.829079632 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "2025-09-24 20:03:29.326235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-09-24 20:03:29.337797: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758744209.349544     248 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758744209.356577     248 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758744209.360819     249 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758744209.367834     249 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "[2025-09-24 20:03:36,321] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-24 20:03:36,321] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\r\n",
      "[2025-09-24 20:03:37,653] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "[2025-09-24 20:03:37,654] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\r\n",
      "[2025-09-24 20:03:37,676] [INFO] [comm.py:821:init_distributed] cdb=None\r\n",
      "[2025-09-24 20:03:37,678] [INFO] [comm.py:821:init_distributed] cdb=None\r\n",
      "[2025-09-24 20:03:37,678] [INFO] [comm.py:852:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\r\n",
      "[W924 20:03:45.985282080 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W924 20:03:45.988209864 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W924 20:03:53.986965490 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W924 20:04:01.989538010 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [01:13<00:00, 36.65s/it]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [01:13<00:00, 36.65s/it]\r\n",
      "Adding EOS to train dataset: 100%|██████| 20/20 [00:00<00:00, 356.49 examples/s]\r\n",
      "Tokenizing train dataset: 100%|█████████| 20/20 [00:00<00:00, 265.35 examples/s]\r\n",
      "Truncating train dataset: 100%|████████| 20/20 [00:00<00:00, 3394.96 examples/s]\r\n",
      "Adding EOS to train dataset: 100%|█████| 20/20 [00:00<00:00, 4411.81 examples/s]\r\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\r\n",
      "Tokenizing train dataset: 100%|█████████| 20/20 [00:00<00:00, 616.51 examples/s]\r\n",
      "Truncating train dataset: 100%|████████| 20/20 [00:00<00:00, 6657.63 examples/s]\r\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\r\n",
      "[2025-09-24 20:05:23,826] [WARNING] [engine.py:1373:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\r\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\r\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\r\n",
      "{'train_runtime': 15.8641, 'train_samples_per_second': 1.261, 'train_steps_per_second': 0.063, 'train_loss': 20.555612564086914, 'num_tokens': 4573.0, 'mean_token_accuracy': 0.0, 'epoch': 1.0}\r\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:15<00:00, 15.86s/it]\r\n",
      "[rank0]:[W924 20:05:46.609977917 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch --config_file accelerate_config.yaml train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13f38726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T20:05:51.610497Z",
     "iopub.status.busy": "2025-09-24T20:05:51.609775Z",
     "iopub.status.idle": "2025-09-24T20:06:24.662655Z",
     "shell.execute_reply": "2025-09-24T20:06:24.661788Z"
    },
    "papermill": {
     "duration": 33.060268,
     "end_time": "2025-09-24T20:06:24.664354",
     "exception": false,
     "start_time": "2025-09-24T20:05:51.604086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Worker 0] Running on GPU 0, data size=5\r\n",
      "[Worker 1] Running on GPU 1, data size=5\r\n",
      "2025-09-24 20:05:58.753244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-09-24 20:05:58.753244: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758744358.779825     423 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758744358.779991     421 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758744358.788163     421 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "E0000 00:00:1758744358.788165     423 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 09-24 20:06:03 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "INFO 09-24 20:06:03 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "WARNING 09-24 20:06:20 [config.py:3443] Casting torch.bfloat16 to torch.float16.\r\n",
      "WARNING 09-24 20:06:20 [config.py:3443] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 09-24 20:06:20 [config.py:1604] Using max model len 2836\r\n",
      "INFO 09-24 20:06:20 [config.py:1604] Using max model len 2836\r\n",
      "WARNING 09-24 20:06:20 [config.py:1084] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 09-24 20:06:20 [config.py:1084] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 09-24 20:06:22 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "WARNING 09-24 20:06:22 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "Process Process-3:\r\n",
      "Process Process-2:\r\n",
      "Traceback (most recent call last):\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n",
      "    self.run()\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 314, in _bootstrap\r\n",
      "    self.run()\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n",
      "    self._target(*self._args, **self._kwargs)\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 108, in run\r\n",
      "    self._target(*self._args, **self._kwargs)\r\n",
      "  File \"/kaggle/working/inference.py\", line 70, in worker\r\n",
      "    preds = run_inference_on_device(df_slice)\r\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/kaggle/working/inference.py\", line 70, in worker\r\n",
      "    preds = run_inference_on_device(df_slice)\r\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/kaggle/working/inference.py\", line 17, in run_inference_on_device\r\n",
      "    llm = vllm.LLM(\r\n",
      "          ^^^^^^^^^\r\n",
      "  File \"/kaggle/working/inference.py\", line 17, in run_inference_on_device\r\n",
      "    llm = vllm.LLM(\r\n",
      "          ^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\", line 273, in __init__\r\n",
      "    self.llm_engine = LLMEngine.from_engine_args(\r\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/entrypoints/llm.py\", line 273, in __init__\r\n",
      "    self.llm_engine = LLMEngine.from_engine_args(\r\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\", line 490, in from_engine_args\r\n",
      "    vllm_config = engine_args.create_engine_config(usage_context)\r\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/llm_engine.py\", line 490, in from_engine_args\r\n",
      "    vllm_config = engine_args.create_engine_config(usage_context)\r\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 1277, in create_engine_config\r\n",
      "    config = VllmConfig(\r\n",
      "             ^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/vllm/engine/arg_utils.py\", line 1277, in create_engine_config\r\n",
      "    config = VllmConfig(\r\n",
      "             ^^^^^^^^^^^\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_dataclasses.py\", line 123, in __init__\r\n",
      "    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\r\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_dataclasses.py\", line 123, in __init__\r\n",
      "    s.__pydantic_validator__.validate_python(ArgsKwargs(args, kwargs), self_instance=s)\r\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for VllmConfig\r\n",
      "  Value error, Cannot find the config file for gptq [type=value_error, input_value=ArgsKwargs((), {'model_co...additional_config': {}}), input_type=ArgsKwargs]\r\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\r\n",
      "pydantic_core._pydantic_core.ValidationError: 1 validation error for VllmConfig\r\n",
      "  Value error, Cannot find the config file for gptq [type=value_error, input_value=ArgsKwargs((), {'model_co...additional_config': {}}), input_type=ArgsKwargs]\r\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/value_error\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/inference.py\", line 120, in <module>\r\n",
      "    main()\r\n",
      "  File \"/kaggle/working/inference.py\", line 108, in main\r\n",
      "    predictions = pd.concat([return_dict[0], return_dict[1]], ignore_index=True)\r\n",
      "                             ~~~~~~~~~~~^^^\r\n",
      "  File \"<string>\", line 2, in __getitem__\r\n",
      "  File \"/usr/lib/python3.11/multiprocessing/managers.py\", line 837, in _callmethod\r\n",
      "    raise convert_to_error(kind, result)\r\n",
      "KeyError: 0\r\n"
     ]
    }
   ],
   "source": [
    "!python inference.py\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "modelId": 322000,
     "modelInstanceId": 301511,
     "sourceId": 363131,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 228.804234,
   "end_time": "2025-09-24T20:06:24.987650",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-24T20:02:36.183416",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
